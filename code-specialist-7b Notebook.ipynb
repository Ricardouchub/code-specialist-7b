{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1911313",
   "metadata": {},
   "source": [
    "### Test de compatibilidad de versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verificación de Versiones de Librerías ---\n",
      "PyTorch:         2.6.0+cu124\n",
      "CUDA (PyTorch):  12.4\n",
      "Transformers:    4.56.2\n",
      "Datasets:        4.1.1\n",
      "PEFT:            0.17.1\n",
      "TRL:             0.23.0\n",
      "BitsAndBytes:    0.47.0\n",
      "Accelerate:      1.10.1\n",
      "-----------------------------------------------\n",
      "\n",
      "CUDA disponible para PyTorch: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import os\n",
    "\n",
    "# Suprimir una advertencia común de bitsandbytes en Windows\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "print(\"--- Verificación de Versiones de Librerías ---\")\n",
    "print(f\"PyTorch:         {torch.__version__}\")\n",
    "print(f\"CUDA (PyTorch):  {torch.version.cuda}\")\n",
    "print(f\"Transformers:    {transformers.__version__}\")\n",
    "print(f\"Datasets:        {datasets.__version__}\")\n",
    "print(f\"PEFT:            {peft.__version__}\")\n",
    "print(f\"TRL:             {trl.__version__}\")\n",
    "print(f\"BitsAndBytes:    {bitsandbytes.__version__}\")\n",
    "print(f\"Accelerate:      {accelerate.__version__}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Verificación final de GPU\n",
    "print(f\"\\nCUDA disponible para PyTorch: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf578d",
   "metadata": {},
   "source": [
    "## Cargar modelo Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc89e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas con éxito.\n",
      "Cargando el modelo base: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Tokenizador cargado.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb34cc1bc28746749168337dc4c08b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo mistralai/Mistral-7B-Instruct-v0.3 cargado en 4-bits.\n",
      "Configuración de LoRA (PEFT) aplicada al modelo.\n",
      "trainable params: 13,631,488 || all params: 7,261,655,040 || trainable%: 0.1877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Suprimir la advertencia de bitsandbytes\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "print(\"Librerías importadas con éxito.\")\n",
    "\n",
    "# --- 1. Definir el Modelo y el Tokenizador (¡CAMBIADO!) ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "NUEVO_MODELO_NOMBRE = \"Mistral-7B-Python-Expert-v1\"\n",
    "\n",
    "print(f\"Cargando el modelo base: {MODEL_ID}\")\n",
    "\n",
    "# --- 2. Configuración de Cuantización (4-bit) ---\n",
    "# Esto es lo que hace que el modelo quepa en tus 12GB de VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- 3. Cargar el Tokenizador ---\n",
    "# No se necesita token. ¡Descarga directa!\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# Mistral, al igual que Llama, no tiene un pad_token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizador cargado.\")\n",
    "\n",
    "# --- 4. Cargar el Modelo Base con Cuantización ---\n",
    "# No se necesita token. ¡Descarga directa!\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # \"auto\" le dice a transformers que ponga el modelo en la GPU\n",
    ")\n",
    "\n",
    "print(f\"Modelo {MODEL_ID} cargado en 4-bits.\")\n",
    "\n",
    "# --- 5. Configurar LoRA (PEFT) ---\n",
    "# Las capas de Mistral 7B son las mismas que las de Llama 3, así que esto no cambia.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- 6. Aplicar LoRA al Modelo ---\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Configuración de LoRA (PEFT) aplicada al modelo.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1412d6",
   "metadata": {},
   "source": [
    "## Cargar y Procesar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81014238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento (Solo Datasets de Código)\n",
      "Cargando CodeAlpaca...\n",
      "CodeAlpaca: 20022 originales -> 8578 de Python\n",
      "Cargando CodeInstructions122k...\n",
      "CodeInstructions122k: 121959 originales -> 63841 de Python\n",
      "Aplicando formato Mistral a los datasets...\n",
      "\n",
      "--- ¡Procesamiento completado! ---\n",
      "Tamaño total del dataset final: 72418\n",
      "\n",
      "--- Ejemplo de formato (índice 0): ---\n",
      "[INST] Create a Python function that takes a dictionary as an argument, and returns the value with maximum frequency in that dictionary.\n",
      "\n",
      "Input:\n",
      "dic = {1: 'a', 2: 'b', 3: 'b', 4: 'c'} [/INST] def max_freq_val(dic): \n",
      "    max_freq = 0\n",
      "    max_val = None\n",
      "    for val in dic.values():\n",
      "        if dic.values().count(val) > max_freq:\n",
      "            max_freq = dic.values().count(val)\n",
      "            max_val = val\n",
      "    \n",
      "    return max_val\n",
      "\n",
      "max_val = max_freq_val(dic)\n",
      "print(max_val)</s>\n",
      "\n",
      "--- Ejemplo de formato (índice 500): ---\n",
      "[INST] Create an algorithm to merge two sorted linked lists into one sorted list. [/INST] def merge_sorted_lists(lst1, lst2):\n",
      "    head = tail = ListNode(None)\n",
      "\n",
      "    while lst1 and lst2:\n",
      "        if lst1.val < lst2.val:\n",
      "            tail.next, lst1 = lst1, lst1.next\n",
      "        else:\n",
      "            tail.next, lst2 = lst2, lst2.next\n",
      "        tail = tail.next\n",
      "\n",
      "    tail.next = lst1 or lst2\n",
      "\n",
      "    return head.next</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import re\n",
    "\n",
    "print(\"Iniciando procesamiento (Solo Datasets de Código)\")\n",
    "\n",
    "# --- 1. Definir un filtro de Python simple y de alta precisión ---\n",
    "PYTHON_KEYWORDS_SIMPLE = set([\"python\", \"pandas\", \"numpy\", \"def \", \"sklearn\", \"torch\", \"tensorflow\",\"sql\"])\n",
    "\n",
    "def es_python_simple(example):\n",
    "    # Combinamos instruction y output\n",
    "    texto_completo = (example['instruction'] + example['output']).lower()\n",
    "    return any(keyword in texto_completo for keyword in PYTHON_KEYWORDS_SIMPLE)\n",
    "\n",
    "# --- 2. Cargar y Filtrar CodeAlpaca ---\n",
    "print(\"Cargando CodeAlpaca...\")\n",
    "ds_alpaca = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n",
    "ds_alpaca_python = ds_alpaca.filter(es_python_simple)\n",
    "print(f\"CodeAlpaca: {len(ds_alpaca)} originales -> {len(ds_alpaca_python)} de Python\")\n",
    "\n",
    "# --- 3. Cargar y Filtrar CodeInstructions122k ---\n",
    "print(\"Cargando CodeInstructions122k...\")\n",
    "ds_code_122k = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "ds_code_122k_python = ds_code_122k.filter(es_python_simple)\n",
    "print(f\"CodeInstructions122k: {len(ds_code_122k)} originales -> {len(ds_code_122k_python)} de Python\")\n",
    "\n",
    "# --- 4. Formatear para SFT (Formato Mistral) ---\n",
    "# Ambos datasets usan el formato 'instruction', 'input', 'output' de Alpaca\n",
    "def format_prompt_mistral_alpaca(example):\n",
    "    instruccion = example['instruction']\n",
    "    input_opcional = example.get('input') # .get() no da error si 'input' no existe\n",
    "    respuesta = example['output']\n",
    "\n",
    "    # Si hay un 'input' (contexto adicional), lo unimos a la instrucción\n",
    "    if input_opcional:\n",
    "        instruccion = instruccion + \"\\n\\nInput:\\n\" + input_opcional\n",
    "    \n",
    "    # Descartar ejemplos sin respuesta\n",
    "    if not respuesta:\n",
    "        return None\n",
    "\n",
    "    example['text'] = f\"[INST] {instruccion} [/INST] {respuesta}</s>\"\n",
    "    return example\n",
    "\n",
    "print(\"Aplicando formato Mistral a los datasets...\")\n",
    "# Usamos 'remove_columns' para limpiar los datasets antes de concatenar\n",
    "ds_alpaca_formatted = ds_alpaca_python.map(format_prompt_mistral_alpaca, remove_columns=['instruction', 'input', 'output'])\n",
    "\n",
    "# --- ¡LÍNEA CORREGIDA ABAJO! ---\n",
    "ds_code_122k_formatted = ds_code_122k_python.map(format_prompt_mistral_alpaca, remove_columns=['instruction', 'input', 'output']) # <-- ¡CORREGIDO!\n",
    "\n",
    "# Filtrar cualquier ejemplo que haya devuelto None\n",
    "ds_alpaca_formatted = ds_alpaca_formatted.filter(lambda x: x['text'] is not None)\n",
    "ds_code_122k_formatted = ds_code_122k_formatted.filter(lambda x: x['text'] is not None)\n",
    "\n",
    "# --- 5. Combinar y Mezclar ---\n",
    "dataset_final = concatenate_datasets([ds_alpaca_formatted, ds_code_122k_formatted])\n",
    "dataset_final = dataset_final.shuffle(seed=42)\n",
    "\n",
    "print(\"\\n--- ¡Procesamiento completado! ---\")\n",
    "print(f\"Tamaño total del dataset final: {len(dataset_final)}\")\n",
    "\n",
    "# Imprimir un ejemplo para verificar el formato\n",
    "print(\"\\n--- Ejemplo de formato (índice 0): ---\")\n",
    "print(dataset_final[0]['text'])\n",
    "print(\"\\n--- Ejemplo de formato (índice 500): ---\")\n",
    "print(dataset_final[500]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db70921",
   "metadata": {},
   "source": [
    "## Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657119df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split] Train: 68797 | Eval: 3621\n",
      "[Precision] bf16=True fp16=False\n",
      "[Eval key] Usando 'eval_strategy' para estrategia de evaluación.\n",
      "[Resume] Reanudando desde: Mistral-7B-Python-Expert-v1\\checkpoint-7000\n",
      "[Train] ¡Iniciando entrenamiento!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\envs\\nlp311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8599' max='8600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8599/8600 3:47:16 < 00:08, 0.12 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.798600</td>\n",
       "      <td>1.751899</td>\n",
       "      <td>0.495269</td>\n",
       "      <td>1301355.000000</td>\n",
       "      <td>0.747835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\envs\\nlp311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Evaluación final...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='453' max='453' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [453/453 19:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval_loss: 1.751899\n",
      "  eval_runtime: 1181.960000\n",
      "  eval_samples_per_second: 3.064000\n",
      "  eval_steps_per_second: 0.383000\n",
      "  eval_entropy: 0.495269\n",
      "  eval_num_tokens: 2077547.000000\n",
      "  eval_mean_token_accuracy: 0.747835\n",
      "  epoch: 0.999913\n",
      "  Perplexity: 5.77\n",
      "[Save] Adaptadores LoRA guardados en: Mistral-7B-Python-Expert-v1-final\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, torch, inspect\n",
    "from datasets import DatasetDict\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ---------- 1) Split train/eval ----------\n",
    "splits = dataset_final.train_test_split(test_size=0.05, seed=42)  # 5% eval\n",
    "train_ds = splits[\"train\"]\n",
    "eval_ds  = splits[\"test\"]\n",
    "print(f\"[Split] Train: {len(train_ds)} | Eval: {len(eval_ds)}\")\n",
    "\n",
    "# ---------- 2) Precisión coherente con tu GPU ----------\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "precision_kwargs = dict(bf16=True, fp16=False) if use_bf16 else dict(bf16=False, fp16=True)\n",
    "print(f\"[Precision] bf16={precision_kwargs.get('bf16')} fp16={precision_kwargs.get('fp16')}\")\n",
    "\n",
    "# ---------- 3) Hiperparámetros ajustables ----------\n",
    "PER_DEVICE_BS = 2\n",
    "GRAD_ACCUM    = 4\n",
    "PACKING       = False\n",
    "MAX_LEN       = 1024\n",
    "LOG_STEPS     = 50\n",
    "SAVE_STEPS    = 1000\n",
    "EVAL_STEPS    = SAVE_STEPS\n",
    "SAVE_LIMIT    = 2\n",
    "\n",
    "# ---------- 4) Detectar nombre correcto del parámetro de evaluación ----------\n",
    "sft_params = set(inspect.signature(SFTConfig.__init__).parameters.keys())\n",
    "EVAL_KEY = \"evaluation_strategy\" if \"evaluation_strategy\" in sft_params else (\n",
    "           \"eval_strategy\"        if \"eval_strategy\"        in sft_params else None)\n",
    "\n",
    "if EVAL_KEY is None:\n",
    "    print(\"[Aviso] Ni 'evaluation_strategy' ni 'eval_strategy' están en SFTConfig; se hará evaluación solo al final.\")\n",
    "else:\n",
    "    print(f\"[Eval key] Usando '{EVAL_KEY}' para estrategia de evaluación.\")\n",
    "\n",
    "# ---------- 5) Construir kwargs comunes de SFTConfig ----------\n",
    "base_kwargs = dict(\n",
    "    output_dir=NUEVO_MODELO_NOMBRE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_LEN,\n",
    "    packing=PACKING,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    pad_token=tokenizer.pad_token,\n",
    "    eos_token=tokenizer.eos_token,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=42,\n",
    "    **precision_kwargs\n",
    ")\n",
    "\n",
    "# Añadir evaluación por pasos usando la clave correcta (si existe)\n",
    "if EVAL_KEY:\n",
    "    base_kwargs[EVAL_KEY] = \"steps\"\n",
    "    base_kwargs[\"eval_steps\"] = EVAL_STEPS\n",
    "\n",
    "# ---------- 6) Instanciar SFTConfig ----------\n",
    "sft_config = SFTConfig(**base_kwargs)\n",
    "\n",
    "# ---------- 7) Crear SFTTrainer (modelo ya envuelto con LoRA con get_peft_model) ----------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=sft_config,\n",
    "    # tokenizer=tokenizer,  # descomenta solo si tu TRL lo acepta (si no, déjalo así)\n",
    ")\n",
    "\n",
    "# ---------- 8) Reanudar desde último checkpoint si existe ----------\n",
    "last_ckpt = None\n",
    "if os.path.isdir(NUEVO_MODELO_NOMBRE):\n",
    "    cks = [d for d in os.listdir(NUEVO_MODELO_NOMBRE) if d.startswith(\"checkpoint-\")]\n",
    "    if cks:\n",
    "        steps = []\n",
    "        for c in cks:\n",
    "            m = re.findall(r\"checkpoint-(\\d+)\", c)\n",
    "            if m:\n",
    "                steps.append(int(m[0]))\n",
    "        if steps:\n",
    "            last_ckpt = os.path.join(NUEVO_MODELO_NOMBRE, f\"checkpoint-{max(steps)}\")\n",
    "            print(f\"[Resume] Reanudando desde: {last_ckpt}\")\n",
    "\n",
    "# ---------- 9) Entrenar ----------\n",
    "print(\"[Train] ¡Iniciando entrenamiento!\")\n",
    "trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "\n",
    "# ---------- 10) Evaluación final + Perplexity ----------\n",
    "print(\"[Eval] Evaluación final...\")\n",
    "metrics = trainer.evaluate()\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "if \"eval_loss\" in metrics and isinstance(metrics[\"eval_loss\"], float):\n",
    "    try:\n",
    "        ppl = math.exp(metrics[\"eval_loss\"])\n",
    "        print(f\"  Perplexity: {ppl:.2f}\")\n",
    "    except OverflowError:\n",
    "        pass\n",
    "\n",
    "# ---------- 11) Guardar adaptadores finales LoRA ----------\n",
    "final_dir = f\"{NUEVO_MODELO_NOMBRE}-final\"\n",
    "trainer.save_model(final_dir)\n",
    "print(f\"[Save] Adaptadores LoRA guardados en: {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9a453",
   "metadata": {},
   "source": [
    "## Merge del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bfcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Adapter: ./Mistral-7B-Python-Expert-v1-final\n",
      "Salida: ./Code-Specialist-7b\n"
     ]
    }
   ],
   "source": [
    "# === MERGE LoRA → MODELO COMPLETO (Notebook-ready) ===\n",
    "import os, gc, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Limpieza básica de memoria\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Rutas\n",
    "BASE_MODEL_ID    = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "ADAPTER_PATH     = \"./Mistral-7B-Python-Expert-v1-final\"     # carpeta de trainer.save_model(...)\n",
    "MERGED_MODEL_DIR = \"./Code-Specialist-7b\"    # salida del modelo fusionado\n",
    "\n",
    "print(\"Base:\", BASE_MODEL_ID)\n",
    "print(\"Adapter:\", ADAPTER_PATH)\n",
    "print(\"Salida:\", MERGED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8eb0127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando base en CPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9ff8b748c748a59cba8c323336f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montando adaptadores LoRA...\n",
      "Fusionando (merge_and_unload)...\n",
      "Merge completo.\n"
     ]
    }
   ],
   "source": [
    "# Cargar base en CPU (sin offload raro)\n",
    "DTYPE_CPU = torch.bfloat16   \n",
    "print(\"Cargando base en CPU...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=DTYPE_CPU,\n",
    "    device_map=None,              # CPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Montando adaptadores LoRA...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "print(\"Fusionando (merge_and_unload)...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(\"Merge completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e693ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando pesos fusionados...\n",
      "Guardando tokenizer...\n",
      "Guardado listo en: ./Code-Specialist-7b\n"
     ]
    }
   ],
   "source": [
    "# Asegurar carpeta de salida\n",
    "os.makedirs(MERGED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Guardado en safetensors (recomendado)\n",
    "print(\"Guardando pesos fusionados...\")\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR, safe_serialization=True)\n",
    "\n",
    "# Guardar tokenizer con pad_token configurado\n",
    "print(\"Guardando tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.save_pretrained(MERGED_MODEL_DIR)\n",
    "\n",
    "# Persistir dtype preferido en config\n",
    "try:\n",
    "    merged_model.config.torch_dtype = str(merged_model.dtype).replace(\"torch.\", \"\")\n",
    "    merged_model.config.save_pretrained(MERGED_MODEL_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Aviso: no se pudo persistir torch_dtype en config:\", e)\n",
    "\n",
    "print(\"Guardado listo en:\", MERGED_MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
