{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1911313",
   "metadata": {},
   "source": [
    "### Test de compatibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee2325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verificación de Versiones de Librerías ---\n",
      "PyTorch:         2.6.0+cu124\n",
      "CUDA (PyTorch):  12.4\n",
      "Transformers:    4.56.2\n",
      "Datasets:        4.1.1\n",
      "PEFT:            0.17.1\n",
      "TRL:             0.23.0\n",
      "BitsAndBytes:    0.47.0\n",
      "Accelerate:      1.10.1\n",
      "-----------------------------------------------\n",
      "\n",
      "CUDA disponible para PyTorch: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import os\n",
    "\n",
    "# Suprimir una advertencia común de bitsandbytes en Windows\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "print(\"--- Verificación de Versiones de Librerías ---\")\n",
    "print(f\"PyTorch:         {torch.__version__}\")\n",
    "print(f\"CUDA (PyTorch):  {torch.version.cuda}\")\n",
    "print(f\"Transformers:    {transformers.__version__}\")\n",
    "print(f\"Datasets:        {datasets.__version__}\")\n",
    "print(f\"PEFT:            {peft.__version__}\")\n",
    "print(f\"TRL:             {trl.__version__}\")\n",
    "print(f\"BitsAndBytes:    {bitsandbytes.__version__}\")\n",
    "print(f\"Accelerate:      {accelerate.__version__}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Verificación final de GPU (redundante pero no hace daño)\n",
    "print(f\"\\nCUDA disponible para PyTorch: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853556a",
   "metadata": {},
   "source": [
    "## Inicio Sesion HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e1fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Sesión iniciada en Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Pega tu token de Hugging Face aquí\n",
    "# Es un token de \"lectura\", así que es seguro\n",
    "TOKEN = \"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\"\n",
    "\n",
    "login(token=TOKEN)\n",
    "\n",
    "print(\"¡Sesión iniciada en Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf578d",
   "metadata": {},
   "source": [
    "## Cargar modelo Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc89e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas con éxito.\n",
      "Cargando el modelo base: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Tokenizador cargado.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb34cc1bc28746749168337dc4c08b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo mistralai/Mistral-7B-Instruct-v0.3 cargado en 4-bits.\n",
      "Configuración de LoRA (PEFT) aplicada al modelo.\n",
      "trainable params: 13,631,488 || all params: 7,261,655,040 || trainable%: 0.1877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Suprimir la advertencia de bitsandbytes\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "print(\"Librerías importadas con éxito.\")\n",
    "\n",
    "# --- 1. Definir el Modelo y el Tokenizador (¡CAMBIADO!) ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "NUEVO_MODELO_NOMBRE = \"Mistral-7B-Python-Expert-v1\"\n",
    "\n",
    "print(f\"Cargando el modelo base: {MODEL_ID}\")\n",
    "\n",
    "# --- 2. Configuración de Cuantización (4-bit) ---\n",
    "# Esto es lo que hace que el modelo quepa en tus 12GB de VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- 3. Cargar el Tokenizador ---\n",
    "# No se necesita token. ¡Descarga directa!\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# Mistral, al igual que Llama, no tiene un pad_token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizador cargado.\")\n",
    "\n",
    "# --- 4. Cargar el Modelo Base con Cuantización ---\n",
    "# No se necesita token. ¡Descarga directa!\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # \"auto\" le dice a transformers que ponga el modelo en la GPU\n",
    ")\n",
    "\n",
    "print(f\"Modelo {MODEL_ID} cargado en 4-bits.\")\n",
    "\n",
    "# --- 5. Configurar LoRA (PEFT) ---\n",
    "# Las capas de Mistral 7B son las mismas que las de Llama 3, así que esto no cambia.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- 6. Aplicar LoRA al Modelo ---\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Configuración de LoRA (PEFT) aplicada al modelo.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1412d6",
   "metadata": {},
   "source": [
    "## Celda 2: Cargar y Procesar los Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81014238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento (Solo Datasets de Código)\n",
      "Cargando CodeAlpaca...\n",
      "CodeAlpaca: 20022 originales -> 8578 de Python\n",
      "Cargando CodeInstructions122k...\n",
      "CodeInstructions122k: 121959 originales -> 63841 de Python\n",
      "Aplicando formato Mistral a los datasets...\n",
      "\n",
      "--- ¡Procesamiento completado! ---\n",
      "Tamaño total del dataset final: 72418\n",
      "\n",
      "--- Ejemplo de formato (índice 0): ---\n",
      "[INST] Create a Python function that takes a dictionary as an argument, and returns the value with maximum frequency in that dictionary.\n",
      "\n",
      "Input:\n",
      "dic = {1: 'a', 2: 'b', 3: 'b', 4: 'c'} [/INST] def max_freq_val(dic): \n",
      "    max_freq = 0\n",
      "    max_val = None\n",
      "    for val in dic.values():\n",
      "        if dic.values().count(val) > max_freq:\n",
      "            max_freq = dic.values().count(val)\n",
      "            max_val = val\n",
      "    \n",
      "    return max_val\n",
      "\n",
      "max_val = max_freq_val(dic)\n",
      "print(max_val)</s>\n",
      "\n",
      "--- Ejemplo de formato (índice 500): ---\n",
      "[INST] Create an algorithm to merge two sorted linked lists into one sorted list. [/INST] def merge_sorted_lists(lst1, lst2):\n",
      "    head = tail = ListNode(None)\n",
      "\n",
      "    while lst1 and lst2:\n",
      "        if lst1.val < lst2.val:\n",
      "            tail.next, lst1 = lst1, lst1.next\n",
      "        else:\n",
      "            tail.next, lst2 = lst2, lst2.next\n",
      "        tail = tail.next\n",
      "\n",
      "    tail.next = lst1 or lst2\n",
      "\n",
      "    return head.next</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import re\n",
    "\n",
    "print(\"Iniciando procesamiento (Solo Datasets de Código)\")\n",
    "\n",
    "# --- 1. Definir un filtro de Python simple y de alta precisión ---\n",
    "PYTHON_KEYWORDS_SIMPLE = set([\"python\", \"pandas\", \"numpy\", \"def \", \"sklearn\", \"torch\", \"tensorflow\",\"sql\"])\n",
    "\n",
    "def es_python_simple(example):\n",
    "    # Combinamos instruction y output\n",
    "    texto_completo = (example['instruction'] + example['output']).lower()\n",
    "    return any(keyword in texto_completo for keyword in PYTHON_KEYWORDS_SIMPLE)\n",
    "\n",
    "# --- 2. Cargar y Filtrar CodeAlpaca ---\n",
    "print(\"Cargando CodeAlpaca...\")\n",
    "ds_alpaca = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n",
    "ds_alpaca_python = ds_alpaca.filter(es_python_simple)\n",
    "print(f\"CodeAlpaca: {len(ds_alpaca)} originales -> {len(ds_alpaca_python)} de Python\")\n",
    "\n",
    "# --- 3. Cargar y Filtrar CodeInstructions122k ---\n",
    "print(\"Cargando CodeInstructions122k...\")\n",
    "ds_code_122k = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "ds_code_122k_python = ds_code_122k.filter(es_python_simple)\n",
    "print(f\"CodeInstructions122k: {len(ds_code_122k)} originales -> {len(ds_code_122k_python)} de Python\")\n",
    "\n",
    "# --- 4. Formatear para SFT (Formato Mistral) ---\n",
    "# Ambos datasets usan el formato 'instruction', 'input', 'output' de Alpaca\n",
    "def format_prompt_mistral_alpaca(example):\n",
    "    instruccion = example['instruction']\n",
    "    input_opcional = example.get('input') # .get() no da error si 'input' no existe\n",
    "    respuesta = example['output']\n",
    "\n",
    "    # Si hay un 'input' (contexto adicional), lo unimos a la instrucción\n",
    "    if input_opcional:\n",
    "        instruccion = instruccion + \"\\n\\nInput:\\n\" + input_opcional\n",
    "    \n",
    "    # Descartar ejemplos sin respuesta\n",
    "    if not respuesta:\n",
    "        return None\n",
    "\n",
    "    example['text'] = f\"[INST] {instruccion} [/INST] {respuesta}</s>\"\n",
    "    return example\n",
    "\n",
    "print(\"Aplicando formato Mistral a los datasets...\")\n",
    "# Usamos 'remove_columns' para limpiar los datasets antes de concatenar\n",
    "ds_alpaca_formatted = ds_alpaca_python.map(format_prompt_mistral_alpaca, remove_columns=['instruction', 'input', 'output'])\n",
    "\n",
    "# --- ¡LÍNEA CORREGIDA ABAJO! ---\n",
    "ds_code_122k_formatted = ds_code_122k_python.map(format_prompt_mistral_alpaca, remove_columns=['instruction', 'input', 'output']) # <-- ¡CORREGIDO!\n",
    "\n",
    "# Filtrar cualquier ejemplo que haya devuelto None\n",
    "ds_alpaca_formatted = ds_alpaca_formatted.filter(lambda x: x['text'] is not None)\n",
    "ds_code_122k_formatted = ds_code_122k_formatted.filter(lambda x: x['text'] is not None)\n",
    "\n",
    "# --- 5. Combinar y Mezclar ---\n",
    "dataset_final = concatenate_datasets([ds_alpaca_formatted, ds_code_122k_formatted])\n",
    "dataset_final = dataset_final.shuffle(seed=42)\n",
    "\n",
    "print(\"\\n--- ¡Procesamiento completado! ---\")\n",
    "print(f\"Tamaño total del dataset final: {len(dataset_final)}\")\n",
    "\n",
    "# Imprimir un ejemplo para verificar el formato\n",
    "print(\"\\n--- Ejemplo de formato (índice 0): ---\")\n",
    "print(dataset_final[0]['text'])\n",
    "print(\"\\n--- Ejemplo de formato (índice 500): ---\")\n",
    "print(dataset_final[500]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db70921",
   "metadata": {},
   "source": [
    "## Celda 3: Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657119df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split] Train: 68797 | Eval: 3621\n",
      "[Precision] bf16=True fp16=False\n",
      "[Eval key] Usando 'eval_strategy' para estrategia de evaluación.\n",
      "[Resume] Reanudando desde: Mistral-7B-Python-Expert-v1\\checkpoint-7000\n",
      "[Train] ¡Iniciando entrenamiento!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\envs\\nlp311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8599' max='8600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8599/8600 3:47:16 < 00:08, 0.12 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.798600</td>\n",
       "      <td>1.751899</td>\n",
       "      <td>0.495269</td>\n",
       "      <td>1301355.000000</td>\n",
       "      <td>0.747835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\anaconda3\\envs\\nlp311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Evaluación final...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='453' max='453' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [453/453 19:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval_loss: 1.751899\n",
      "  eval_runtime: 1181.960000\n",
      "  eval_samples_per_second: 3.064000\n",
      "  eval_steps_per_second: 0.383000\n",
      "  eval_entropy: 0.495269\n",
      "  eval_num_tokens: 2077547.000000\n",
      "  eval_mean_token_accuracy: 0.747835\n",
      "  epoch: 0.999913\n",
      "  Perplexity: 5.77\n",
      "[Save] Adaptadores LoRA guardados en: Mistral-7B-Python-Expert-v1-final\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENTRENAMIENTO SFT ROBUSTO (TRL 0.23 + PEFT)\n",
    "# Auto-fallback para eval_strategy / evaluation_strategy\n",
    "# ============================================\n",
    "\n",
    "import os, re, math, torch, inspect\n",
    "from datasets import DatasetDict\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ---------- 1) Split train/eval ----------\n",
    "splits = dataset_final.train_test_split(test_size=0.05, seed=42)  # 5% eval\n",
    "train_ds = splits[\"train\"]\n",
    "eval_ds  = splits[\"test\"]\n",
    "print(f\"[Split] Train: {len(train_ds)} | Eval: {len(eval_ds)}\")\n",
    "\n",
    "# ---------- 2) Precisión coherente con tu GPU ----------\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "precision_kwargs = dict(bf16=True, fp16=False) if use_bf16 else dict(bf16=False, fp16=True)\n",
    "print(f\"[Precision] bf16={precision_kwargs.get('bf16')} fp16={precision_kwargs.get('fp16')}\")\n",
    "\n",
    "# ---------- 3) Hiperparámetros ajustables ----------\n",
    "PER_DEVICE_BS = 2\n",
    "GRAD_ACCUM    = 4\n",
    "PACKING       = False\n",
    "MAX_LEN       = 1024\n",
    "LOG_STEPS     = 50\n",
    "SAVE_STEPS    = 1000\n",
    "EVAL_STEPS    = SAVE_STEPS\n",
    "SAVE_LIMIT    = 2\n",
    "\n",
    "# ---------- 4) Detectar nombre correcto del parámetro de evaluación ----------\n",
    "sft_params = set(inspect.signature(SFTConfig.__init__).parameters.keys())\n",
    "EVAL_KEY = \"evaluation_strategy\" if \"evaluation_strategy\" in sft_params else (\n",
    "           \"eval_strategy\"        if \"eval_strategy\"        in sft_params else None)\n",
    "\n",
    "if EVAL_KEY is None:\n",
    "    print(\"[Aviso] Ni 'evaluation_strategy' ni 'eval_strategy' están en SFTConfig; se hará evaluación solo al final.\")\n",
    "else:\n",
    "    print(f\"[Eval key] Usando '{EVAL_KEY}' para estrategia de evaluación.\")\n",
    "\n",
    "# ---------- 5) Construir kwargs comunes de SFTConfig ----------\n",
    "base_kwargs = dict(\n",
    "    output_dir=NUEVO_MODELO_NOMBRE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_LEN,\n",
    "    packing=PACKING,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    pad_token=tokenizer.pad_token,\n",
    "    eos_token=tokenizer.eos_token,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=42,\n",
    "    **precision_kwargs\n",
    ")\n",
    "\n",
    "# Añadir evaluación por pasos usando la clave correcta (si existe)\n",
    "if EVAL_KEY:\n",
    "    base_kwargs[EVAL_KEY] = \"steps\"\n",
    "    base_kwargs[\"eval_steps\"] = EVAL_STEPS\n",
    "\n",
    "# ---------- 6) Instanciar SFTConfig ----------\n",
    "sft_config = SFTConfig(**base_kwargs)\n",
    "\n",
    "# ---------- 7) Crear SFTTrainer (modelo ya envuelto con LoRA con get_peft_model) ----------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    args=sft_config,\n",
    "    # tokenizer=tokenizer,  # descomenta solo si tu TRL lo acepta (si no, déjalo así)\n",
    ")\n",
    "\n",
    "# ---------- 8) Reanudar desde último checkpoint si existe ----------\n",
    "last_ckpt = None\n",
    "if os.path.isdir(NUEVO_MODELO_NOMBRE):\n",
    "    cks = [d for d in os.listdir(NUEVO_MODELO_NOMBRE) if d.startswith(\"checkpoint-\")]\n",
    "    if cks:\n",
    "        steps = []\n",
    "        for c in cks:\n",
    "            m = re.findall(r\"checkpoint-(\\d+)\", c)\n",
    "            if m:\n",
    "                steps.append(int(m[0]))\n",
    "        if steps:\n",
    "            last_ckpt = os.path.join(NUEVO_MODELO_NOMBRE, f\"checkpoint-{max(steps)}\")\n",
    "            print(f\"[Resume] Reanudando desde: {last_ckpt}\")\n",
    "\n",
    "# ---------- 9) Entrenar ----------\n",
    "print(\"[Train] ¡Iniciando entrenamiento!\")\n",
    "trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "\n",
    "# ---------- 10) Evaluación final + Perplexity ----------\n",
    "print(\"[Eval] Evaluación final...\")\n",
    "metrics = trainer.evaluate()\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "if \"eval_loss\" in metrics and isinstance(metrics[\"eval_loss\"], float):\n",
    "    try:\n",
    "        ppl = math.exp(metrics[\"eval_loss\"])\n",
    "        print(f\"  Perplexity: {ppl:.2f}\")\n",
    "    except OverflowError:\n",
    "        pass\n",
    "\n",
    "# ---------- 11) Guardar adaptadores finales LoRA ----------\n",
    "final_dir = f\"{NUEVO_MODELO_NOMBRE}-final\"\n",
    "trainer.save_model(final_dir)\n",
    "print(f\"[Save] Adaptadores LoRA guardados en: {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e66c9b",
   "metadata": {},
   "source": [
    "## Comparación Cualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb2871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Probando el Modelo Base (Original) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f5772bbc454e8fa0e730ef1d1ef975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPUESTA (Base):\n",
      " Escribe una función en Python que use pandas para cargar un archivo CSV llamado 'usuarios.csv' y devuelva el promedio de la columna 'edad'.  Para crear una función en Python que utilice la biblioteca pandas para cargar un archivo CSV llamado 'usuarios.csv' y devuelva el promedio de la columna 'edad', sigue este ejemplo:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def promedio_edad(archivo):\n",
      "    # Cargar el archivo CSV\n",
      "    df = pd.read_csv(archivo)\n",
      "\n",
      "    # Calcular el promedio de la columna 'edad'\n",
      "    promedio = df['edad'].mean()\n",
      "\n",
      "    return promedio\n",
      "\n",
      "# Uso de la función\n",
      "archivo = 'usuarios.csv'\n",
      "promedio = promedio_edad(archivo)\n",
      "print(f'El promedio de edad es: {promedio}')\n",
      "```\n",
      "\n",
      "Por último, asegúrate de tener el archivo CSV 'usuarios.csv' en la carpeta donde se ejecuta el código o proporciona la ruta completa del archivo\n",
      "\n",
      "--- Modelo Base limpiado de la VRAM ---\n",
      "\n",
      "--- 2. Probando el Modelo Experto (Afinado) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83764cf914444b049b66e4a9866e406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptadores LoRA cargados.\n",
      "RESPUESTA (Experto):\n",
      " Escribe una función en Python que use pandas para cargar un archivo CSV llamado 'usuarios.csv' y devuelva el promedio de la columna 'edad'.  Para crear una función en Python que utilice la biblioteca pandas para cargar un archivo CSV y devolver el promedio de la columna 'edad', sigue este ejemplo:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def promedio_edad(archivo_csv):\n",
      "    try:\n",
      "        data = pd.read_csv(archivo_csv)\n",
      "        edad = data['edad']\n",
      "        promedio = edad.mean()\n",
      "        return promedio\n",
      "    except FileNotFoundError:\n",
      "        print(\"El archivo no se encuentra en la ruta especificada.\")\n",
      "        return None\n",
      "\n",
      "# Usa la función para cargar el archivo 'usuarios.csv'\n",
      "archivo = 'usuarios.csv'\n",
      "promedio = promedio_edad(archivo)\n",
      "if promedio is not None:\n",
      "    print(f\"El promedio de la columna 'edad' es: {promedio}\")\n",
      "```\n",
      "\n",
      "En este ejemplo, la función `promedio_edad(archivo_\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc # Garbage Collector, para limpiar la memoria\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- 1. Definir Constantes ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# ¡IMPORTANTE! Asegúrate de que este sea el nombre de la carpeta que se guardó\n",
    "ADAPTER_PATH = \"./Mistral-7B-Python-Expert-v1-final\" \n",
    "\n",
    "# --- 2. Configuración de Carga (misma que antes) ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- 3. Definir el Prompt de Prueba ---\n",
    "# ¡Puedes cambiar esta pregunta por la que tú quieras!\n",
    "pregunta = \"Escribe una función en Python que use pandas para cargar un archivo CSV llamado 'usuarios.csv' y devuelva el promedio de la columna 'edad'.\"\n",
    "\n",
    "prompt_formateado = f\"[INST] {pregunta} [/INST]\"\n",
    "\n",
    "# =========================================================\n",
    "# \n",
    "#              PRUEBA 1: MODELO BASE\n",
    "# \n",
    "# =========================================================\n",
    "print(\"--- 1. Probando el Modelo Base (Original) ---\")\n",
    "\n",
    "# Cargar modelo base\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Cargar tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generar respuesta\n",
    "inputs = tokenizer(prompt_formateado, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model_base.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "respuesta_base = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"RESPUESTA (Base):\\n\", respuesta_base)\n",
    "\n",
    "# --- Limpiar VRAM ---\n",
    "del model_base\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n--- Modelo Base limpiado de la VRAM ---\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# \n",
    "#             PRUEBA 2: MODELO EXPERTO (AFINADO)\n",
    "# \n",
    "# =========================================================\n",
    "print(\"\\n--- 2. Probando el Modelo Experto (Afinado) ---\")\n",
    "\n",
    "# Volver a cargar el modelo base (necesario para aplicar LoRA)\n",
    "model_para_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ¡Cargar los adaptadores LoRA encima!\n",
    "model_experto = PeftModel.from_pretrained(model_para_lora, ADAPTER_PATH)\n",
    "model_experto.eval() # Poner en modo de evaluación\n",
    "\n",
    "print(\"Adaptadores LoRA cargados.\")\n",
    "\n",
    "# Generar respuesta (reusamos el tokenizer y el prompt)\n",
    "inputs = tokenizer(prompt_formateado, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs_experto = model_experto.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=256, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "respuesta_experto = tokenizer.decode(outputs_experto[0], skip_special_tokens=True)\n",
    "print(\"RESPUESTA (Experto):\\n\", respuesta_experto)\n",
    "\n",
    "# --- Limpiar VRAM al final ---\n",
    "del model_para_lora\n",
    "del model_experto\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013b6fe",
   "metadata": {},
   "source": [
    "## 1. Evaluación Cuantitativa (Benchmark HumanEval offline) 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando existente: ./humaneval_local_25.jsonl\n",
      "Funciones de HumanEval-Local definidas.\n"
     ]
    }
   ],
   "source": [
    "import json, os, re, tempfile, subprocess, random, time, csv, uuid, textwrap\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Config ---\n",
    "JSONL_PATH = \"./humaneval_local_25.jsonl\" # Lo guardará en el directorio actual\n",
    "RESULTS_CSV_BASE = \"./humaneval_local_results_BASE.csv\"\n",
    "RESULTS_CSV_EXPERTO = \"./humaneval_local_results_EXPERTO.csv\"\n",
    "GEN_KW = dict(max_new_tokens=384, temperature=0.1, top_p=0.95, do_sample=True, pad_token_id=2) # 2 es eos_token_id para Mistral\n",
    "\n",
    "\n",
    "TASKS = [\n",
    "    (\"add\", \"def add(a, b):\\n    \\\"\\\"\\\"Return a + b\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_add():\\n    assert add(1,2)==3\\n    assert add(-1,1)==0\\n\"),\n",
    "    (\"is_even\", \"def is_even(n:int)->bool:\\n    \\\"\\\"\\\"Return True if n is even\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_e():\\n    assert is_even(2)\\n    assert not is_even(3)\\n\"),\n",
    "    (\"fib\", \"def fib(n:int):\\n    \\\"\\\"\\\"Return a list with the first n Fibonacci numbers starting at 0,1\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_f():\\n    assert fib(1)==[0]\\n    assert fib(2)==[0,1]\\n    assert fib(7)==[0,1,1,2,3,5,8]\\n\"),\n",
    "    (\"is_prime\", \"def is_prime(n:int)->bool:\\n    \\\"\\\"\\\"Return True if n is prime\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_p():\\n    assert is_prime(2) and is_prime(3) and is_prime(97)\\n    assert not is_prime(1)\\n    assert not is_prime(100)\\n\"),\n",
    "    (\"flatten\", \"def flatten(lst:list)->list:\\n    \\\"\\\"\\\"Flatten a shallow list of lists\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_flat():\\n    assert flatten([[1,2],[3],[]])==[1,2,3]\\n    assert flatten([])==[]\\n\"),\n",
    "    (\"reverse_words\", \"def reverse_words(s:str)->str:\\n    \\\"\\\"\\\"Reverse word order by spaces\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_rev():\\n    assert reverse_words('hola mundo')=='mundo hola'\\n    assert reverse_words('a b c')=='c b a'\\n\"),\n",
    "    (\"count_vowels\", \"def count_vowels(s:str)->int:\\n    \\\"\\\"\\\"Count lowercase vowels aeiou\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_v():\\n    assert count_vowels('hola')==2\\n    assert count_vowels('xyz')==0\\n\"),\n",
    "    (\"unique_sorted\", \"def unique_sorted(lst:list)->list:\\n    \\\"\\\"\\\"Return sorted unique elements\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_u():\\n    assert unique_sorted([3,1,2,3,2,1])==[1,2,3]\\n\"),\n",
    "    (\"sum_digits\", \"def sum_digits(n:int)->int:\\n    \\\"\\\"\\\"Sum digits of a non-negative integer\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_s():\\n    assert sum_digits(0)==0\\n    assert sum_digits(12345)==15\\n\"),\n",
    "    (\"is_palindrome\", \"def is_palindrome(s:str)->bool:\\n    \\\"\\\"\\\"Check palindrome ignoring spaces and case\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_pal():\\n    assert is_palindrome('Anita lava la tina')\\n    assert not is_palindrome('python')\\n\"),\n",
    "    (\"fact\", \"def fact(n:int)->int:\\n    \\\"\\\"\\\"Return n! for n>=0\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_fact():\\n    assert fact(0)==1\\n    assert fact(5)==120\\n\"),\n",
    "    (\"merge_dicts\", \"def merge_dicts(a:dict,b:dict)->dict:\\n    \\\"\\\"\\\"Merge summing values for common keys\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_m():\\n    assert merge_dicts({'a':1,'b':2},{'b':3,'c':4})=={'a':1,'b':5,'c':4}\\n\"),\n",
    "    (\"gcd\", \"def gcd(a:int,b:int)->int:\\n    \\\"\\\"\\\"Greatest common divisor\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_gcd():\\n    assert gcd(12,18)==6\\n    assert gcd(7,13)==1\\n\"),\n",
    "    (\"two_sum\", \"def two_sum(nums:list, target:int)->tuple:\\n    \\\"\\\"\\\"Return indices (i,j) with nums[i]+nums[j]==target (i<j) or (-1,-1)\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_ts():\\n    assert two_sum([2,7,11,15],9)==(0,1)\\n    assert two_sum([3,2,4],6)==(1,2)\\n\"),\n",
    "    (\"anagrams\", \"def anagrams(a:str,b:str)->bool:\\n    \\\"\\\"\\\"Check if two strings are anagrams (ignore spaces and case)\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_a():\\n    assert anagrams('Listen','Silent')\\n    assert not anagrams('apple','pale')\\n\"),\n",
    "    (\"balanced_parens\", \"def balanced_parens(s:str)->bool:\\n    \\\"\\\"\\\"Return True if parentheses are balanced\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_bp():\\n    assert balanced_parens('(())')\\n    assert not balanced_parens('(()')\\n\"),\n",
    "    (\"roman_to_int\", \"def roman_to_int(s:str)->int:\\n    \\\"\\\"\\\"Convert Roman numeral to int\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_r():\\n    assert roman_to_int('III')==3\\n    assert roman_to_int('IV')==4\\n    assert roman_to_int('IX')==9\\n\"),\n",
    "    (\"int_to_roman\", \"def int_to_roman(n:int)->str:\\n    \\\"\\\"\\\"Convert 1..3999 to Roman numeral\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_ir():\\n    assert int_to_roman(3)=='III'\\n    assert int_to_roman(4)=='IV'\\n    assert int_to_roman(9)=='IX'\\n\"),\n",
    "    (\"top_k\", \"def top_k(lst:list, k:int)->list:\\n    \\\"\\\"\\\"Return k largest elements sorted desc\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_topk():\\n    assert top_k([1,5,2,4,3],2)==[5,4]\\n\"),\n",
    "    (\"unique_chars\", \"def unique_chars(s:str)->bool:\\n    \\\"\\\"\\\"True if all chars are unique\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_uc():\\n    assert unique_chars('abc')\\n    assert not unique_chars('aba')\\n\"),\n",
    "    (\"median\", \"def median(nums:list)->float:\\n    \\\"\\\"\\\"Return median of a non-empty list\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_med():\\n    assert median([1,3,2])==2\\n    assert median([1,2,3,4])==2.5\\n\"),\n",
    "    (\"mode\", \"def mode(nums:list):\\n    \\\"\\\"\\\"Return the most frequent element; tie-break by smallest value\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_mode():\\n    assert mode([1,1,2,2,2,3])==2\\n    assert mode([3,3,1,1])==1\\n\"),\n",
    "    (\"to_snake\", \"def to_snake(s:str)->str:\\n    \\\"\\\"\\\"Convert CamelCase to snake_case\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_snake():\\n    assert to_snake('CamelCase')=='camel_case'\\n    assert to_snake('HTTPServer')=='http_server'\\n\"),\n",
    "    (\"dedup_preserve\", \"def dedup_preserve(lst:list)->list:\\n    \\\"\\\"\\\"Remove duplicates preserving first appearance\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_dedup():\\n    assert dedup_preserve([1,2,1,3,2])==[1,2,3]\\n\"),\n",
    "    (\"chunk\", \"def chunk(lst:list, size:int)->list:\\n    \\\"\\\"\\\"Split list into chunks of given size (>0)\\\"\\\"\\\"\\n\", \"import pytest\\n\\ndef test_chunk():\\n    assert chunk([1,2,3,4,5],2)==[[1,2],[3,4],[5]]\\n\"),\n",
    "]\n",
    "\n",
    "# --- Si no existe el JSONL, lo creamos ---\n",
    "if not Path(JSONL_PATH).exists():\n",
    "    with open(JSONL_PATH,\"w\",encoding=\"utf-8\") as f:\n",
    "        for i,(entry,prompt,test) in enumerate(TASKS):\n",
    "            rec = {\"task_id\": f\"HE_LOCAL/{i}\", \"prompt\": prompt, \"entry_point\": entry, \"test\": test}\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(\"Creado:\", JSONL_PATH)\n",
    "else:\n",
    "    print(\"Usando existente:\", JSONL_PATH)\n",
    "\n",
    "# --- Función para ejecutar Pytest (la tuya, es perfecta) ---\n",
    "def run_pytest(code:str, entry:str, tests:str, timeout=10)->bool:\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        code_path = os.path.join(td, \"user_code.py\")\n",
    "        test_path = os.path.join(td, \"test_code.py\")\n",
    "        \n",
    "        with open(code_path,\"w\",encoding=\"utf-8\") as f: f.write(code)\n",
    "        \n",
    "        with open(test_path,\"w\",encoding=\"utf-8\") as f:\n",
    "            f.write(f\"from user_code import {entry}\\n\")\n",
    "            f.write(tests + \"\\n\")\n",
    "            # f.write(\"if __name__ == '__main__':\\n    import pytest; pytest.main([__file__,'-q'])\\n\") # No es necesario\n",
    "        \n",
    "        try:\n",
    "            # Usar pytest directamente es más robusto\n",
    "            p = subprocess.run([\"pytest\", test_path], capture_output=True, text=True, timeout=timeout)\n",
    "            return p.returncode == 0\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "# --- ¡NUEVA! Función para extraer el código ---\n",
    "def extract_python_code(text, prompt):\n",
    "    # 1. Buscar bloque ```python ... ```\n",
    "    pattern = r\"```python\\n(.*?)\\n```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # 2. Si no, buscar bloque ``` ... ```\n",
    "    pattern = r\"```\\n(.*?)\\n```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # 3. Si no, tomar todo después de [/INST]\n",
    "    if \"[/INST]\" in text:\n",
    "        text_after_inst = text.split(\"[/INST]\", 1)[1].strip()\n",
    "        # Si el modelo es bueno, puede que solo genere el código.\n",
    "        # Si repite el prompt, lo quitamos.\n",
    "        if text_after_inst.startswith(prompt.strip()):\n",
    "            text_after_inst = text_after_inst[len(prompt.strip()):]\n",
    "        # Si empieza con la función (completándola), está bien\n",
    "        if text_after_inst.strip().startswith(\"def\"):\n",
    "             return text_after_inst.strip()\n",
    "        # Si el modelo solo genera la *continuación* del prompt\n",
    "        if not prompt.strip().endswith(\"\\n\"):\n",
    "             prompt += \"\\n\"\n",
    "        return prompt + text_after_inst # Devolver prompt + continuación\n",
    "    \n",
    "    # 4. Fallback: asumir que la respuesta es solo código\n",
    "    return text.strip()\n",
    "\n",
    "# --- ¡MODIFICADA! Función de Evaluación ---\n",
    "def eval_local(model, tokenizer, jsonl_path, results_csv_path):\n",
    "    ok, total = 0, 0\n",
    "    rows = []\n",
    "    \n",
    "    with open(jsonl_path,encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Evaluando tareas\"):\n",
    "            t = json.loads(line)\n",
    "            prompt_file, entry, tests, tid = t[\"prompt\"], t[\"entry_point\"], t[\"test\"], t[\"task_id\"]\n",
    "            \n",
    "            # 1. Formatear el prompt para nuestro Chat Model\n",
    "            prompt_chat = (\n",
    "                f\"[INST] Escribe una función de Python para el siguiente problema. \"\n",
    "                f\"Responde SÓLO con el bloque de código Python, sin explicaciones.\\n\\n\"\n",
    "                f\"```python\\n{prompt_file}\\n```\\n[/INST]\"\n",
    "            )\n",
    "            \n",
    "            inputs = tokenizer(prompt_chat, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # 2. Generar\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, **GEN_KW)\n",
    "            \n",
    "            gen_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            \n",
    "            # 3. Extraer el código\n",
    "            code_extracted = extract_python_code(gen_text, prompt_file)\n",
    "            \n",
    "            # 4. Probar\n",
    "            passed = run_pytest(code_extracted, entry, tests)\n",
    "            \n",
    "            ok += int(passed); total += 1\n",
    "            rows.append({\"task_id\": tid, \"passed\": int(passed)})\n",
    "            # print(f\"{tid}: {'OK' if passed else 'FAIL'}\") # Descomentar para debug\n",
    "    \n",
    "    print(f\"\\nResultados para {model.config._name_or_path}:\")\n",
    "    print(f\"pass@1: {ok}/{total} = {ok/total:.2%}\")\n",
    "    \n",
    "    # 5. Guardar CSV\n",
    "    with open(results_csv_path,\"w\",newline=\"\",encoding=\"utf-8\") as cf:\n",
    "        w = csv.DictWriter(cf, fieldnames=[\"task_id\",\"passed\"]); w.writeheader(); w.writerows(rows)\n",
    "    print(\"CSV guardado en:\", results_csv_path)\n",
    "    return ok/total\n",
    "\n",
    "print(\"Funciones de HumanEval-Local definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a45dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluando Modelo BASE ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f1633ab3d04ff4b97485239828e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando tareas: 25it [06:34, 15.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para mistralai/Mistral-7B-Instruct-v0.3:\n",
      "pass@1: 0/25 = 0.00%\n",
      "CSV guardado en: ./humaneval_local_results_BASE.csv\n",
      "--- Modelo Base limpiado ---\n",
      "\n",
      "--- Evaluando Modelo EXPERTO ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7debd3885d41bb9056783d686258c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando tareas: 25it [08:08, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para mistralai/Mistral-7B-Instruct-v0.3:\n",
      "pass@1: 0/25 = 0.00%\n",
      "CSV guardado en: ./humaneval_local_results_EXPERTO.csv\n",
      "--- Modelo Experto limpiado ---\n",
      "\n",
      "--- ¡Benchmark completado! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- 0. Limpiar Memoria ---\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- 1. Definir Constantes de Carga ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "ADAPTER_PATH = \"./Mistral-7B-Python-Expert-v1-final\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# =========================================================\n",
    "# \n",
    "#              PRUEBA 1: MODELO BASE\n",
    "# \n",
    "# =========================================================\n",
    "print(\"\\n--- Evaluando Modelo BASE ---\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "# Ejecutar la evaluación\n",
    "eval_local(base_model, tokenizer, JSONL_PATH, RESULTS_CSV_BASE)\n",
    "\n",
    "# Limpiar\n",
    "del base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"--- Modelo Base limpiado ---\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# \n",
    "#             PRUEBA 2: MODELO EXPERTO (AFINADO)\n",
    "# \n",
    "# =========================================================\n",
    "print(\"\\n--- Evaluando Modelo EXPERTO ---\")\n",
    "# Volver a cargar el modelo base (necesario para aplicar LoRA)\n",
    "base_model_for_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# ¡Cargar los adaptadores LoRA encima!\n",
    "model_experto = PeftModel.from_pretrained(base_model_for_lora, ADAPTER_PATH)\n",
    "model_experto.eval() # Poner en modo de evaluación\n",
    "\n",
    "# Ejecutar la evaluación\n",
    "eval_local(model_experto, tokenizer, JSONL_PATH, RESULTS_CSV_EXPERTO)\n",
    "\n",
    "# Limpiar\n",
    "del model_experto\n",
    "del base_model_for_lora\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"--- Modelo Experto limpiado ---\")\n",
    "\n",
    "print(\"\\n--- ¡Benchmark completado! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9a453",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bfcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: mistralai/Mistral-7B-Instruct-v0.3\n",
      "Adapter: ./Mistral-7B-Python-Expert-v1-final\n",
      "Salida: ./Code-Specialist-7b\n"
     ]
    }
   ],
   "source": [
    "# === MERGE LoRA → MODELO COMPLETO (Notebook-ready) ===\n",
    "import os, gc, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Limpieza básica de memoria\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Rutas\n",
    "BASE_MODEL_ID    = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "ADAPTER_PATH     = \"./Mistral-7B-Python-Expert-v1-final\"     # carpeta de trainer.save_model(...)\n",
    "MERGED_MODEL_DIR = \"./Code-Specialist-7b\"    # salida del modelo fusionado\n",
    "\n",
    "print(\"Base:\", BASE_MODEL_ID)\n",
    "print(\"Adapter:\", ADAPTER_PATH)\n",
    "print(\"Salida:\", MERGED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8eb0127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando base en CPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9ff8b748c748a59cba8c323336f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montando adaptadores LoRA...\n",
      "Fusionando (merge_and_unload)...\n",
      "Merge completo.\n"
     ]
    }
   ],
   "source": [
    "# Cargar base en CPU (sin offload raro)\n",
    "DTYPE_CPU = torch.bfloat16   \n",
    "print(\"Cargando base en CPU...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=DTYPE_CPU,\n",
    "    device_map=None,              # CPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Montando adaptadores LoRA...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "print(\"Fusionando (merge_and_unload)...\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(\"Merge completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e693ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando pesos fusionados...\n",
      "Guardando tokenizer...\n",
      "Guardado listo en: ./Code-Specialist-7b\n"
     ]
    }
   ],
   "source": [
    "# Asegurar carpeta de salida\n",
    "os.makedirs(MERGED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Guardado en safetensors (recomendado)\n",
    "print(\"Guardando pesos fusionados...\")\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR, safe_serialization=True)\n",
    "\n",
    "# Guardar tokenizer con pad_token configurado\n",
    "print(\"Guardando tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.save_pretrained(MERGED_MODEL_DIR)\n",
    "\n",
    "# Persistir dtype preferido en config\n",
    "try:\n",
    "    merged_model.config.torch_dtype = str(merged_model.dtype).replace(\"torch.\", \"\")\n",
    "    merged_model.config.save_pretrained(MERGED_MODEL_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Aviso: no se pudo persistir torch_dtype en config:\", e)\n",
    "\n",
    "print(\"Guardado listo en:\", MERGED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aeeca",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71a24e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: cargando modelo merged para inferencia...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44b8f00334243da9311873ca7888ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribe una función Python que sume dos enteros y devuelve solo la función.  Aquí tienes una función simple en Python que sume dos enteros:\n",
      "\n",
      "```python\n",
      "def sumar(num1, num2):\n",
      "    return num1 + num2\n",
      "```\n",
      "\n",
      "Puedes usar esta función de la siguiente manera:\n",
      "\n",
      "```python\n",
      "resultado = sumar(5, 3)\n",
      "print(resultado)  # Imprime 8\n",
      "```\n",
      "\n",
      "Esta función toma dos números enteros como parámetros y devuelve la suma de ambos.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Sanity check: cargando modelo merged para inferencia...\")\n",
    "tok = AutoTokenizer.from_pretrained(MERGED_MODEL_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "dtype_infer = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "merged_for_infer = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_MODEL_DIR,\n",
    "    torch_dtype=dtype_infer,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "prompt = \"[INST] Escribe una función Python que sume dos enteros y devuelve solo la función. [/INST]\"\n",
    "batch = tok(prompt, return_tensors=\"pt\").to(merged_for_infer.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = merged_for_infer.generate(\n",
    "        **batch, max_new_tokens=128,\n",
    "        eos_token_id=tok.eos_token_id, pad_token_id=tok.pad_token_id\n",
    "    )\n",
    "\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb9a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando 4-bit forzado en GPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9afaff2782435d8614f86b440f25ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4-bit en GPU cargado.\n",
      "Modelo listo.\n"
     ]
    }
   ],
   "source": [
    "# === Cargar MERGED en 4-bit con fallback robusto ===\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MERGED_MODEL_DIR = \"./Mistral-7B-Python-Expert-v1-MERGED\"\n",
    "\n",
    "# Tokenizer\n",
    "tok4 = AutoTokenizer.from_pretrained(MERGED_MODEL_DIR, use_fast=True)\n",
    "if tok4.pad_token is None:\n",
    "    tok4.pad_token = tok4.eos_token\n",
    "\n",
    "def load_4bit_force_gpu():\n",
    "    \"\"\"Carga 4-bit forzando TODO a la GPU (sin offload a CPU/disk).\"\"\"\n",
    "    assert torch.cuda.is_available(), \"Se requiere GPU para 4-bit.\"\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    bnb4 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    # Fuerza todo en la GPU actual:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MERGED_MODEL_DIR,\n",
    "        quantization_config=bnb4,\n",
    "        device_map={\"\": gpu_id},      # <- evita que \"auto\" mande algo a CPU/disk\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    return model.eval()\n",
    "\n",
    "def load_8bit_with_cpu_offload():\n",
    "    \"\"\"Fallback: 8-bit permite offload FP32 a CPU.\"\"\"\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb8 = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,  # <- int8 sí permite offload en CPU\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MERGED_MODEL_DIR,\n",
    "        quantization_config=bnb8,\n",
    "        device_map=\"auto\",             # puede repartir entre GPU/CPU\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    return model.eval()\n",
    "\n",
    "def load_16bit_if_possible():\n",
    "    \"\"\"Último recurso: sin cuantización (bf16/fp16), depende de tu VRAM.\"\"\"\n",
    "    dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MERGED_MODEL_DIR,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    return model.eval()\n",
    "\n",
    "try:\n",
    "    print(\"Intentando 4-bit forzado en GPU...\")\n",
    "    merged_model = load_4bit_force_gpu()\n",
    "    print(\"✅ 4-bit en GPU cargado.\")\n",
    "except Exception as e4:\n",
    "    print(\"[Aviso] 4-bit en GPU no se pudo cargar:\", type(e4).__name__, \"-\", e4)\n",
    "    try:\n",
    "        print(\"Intentando 8-bit con offload en CPU...\")\n",
    "        merged_model = load_8bit_with_cpu_offload()\n",
    "        print(\"✅ 8-bit con offload cargado.\")\n",
    "    except Exception as e8:\n",
    "        print(\"[Aviso] 8-bit con offload tampoco se pudo cargar:\", type(e8).__name__, \"-\", e8)\n",
    "        print(\"Intentando 16-bit (sin cuantizar)...\")\n",
    "        merged_model = load_16bit_if_possible()\n",
    "        print(\"✅ 16-bit cargado (verifica VRAM).\")\n",
    "\n",
    "print(\"Modelo listo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
